# èŠå¤©æœºå™¨äºº
```shell
pip install protobuf transformers==4.27.1 cpm_kernels
```
## coding
```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("./chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("./chatglm-6b-int4", trust_remote_code=True).half().cuda()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)

# ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
```

## pretrained model
- [é“¾æ¥](https://pan.baidu.com/s/1VPRGReHfnnqe_ULKjquoaQ?pwd=oaae)
- æå–ç ï¼šoaae 

# å‚è€ƒèµ„æ–™
[å‚è€ƒèµ„æ–™2](https://huggingface.co/THUDM/chatglm-6b-int4)
[å‚è€ƒèµ„æ–™](https://github.com/wenda-LLM/wenda/tree/main)