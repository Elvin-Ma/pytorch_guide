# èŠå¤©æœºå™¨äºº

## 1.1 ç¯å¢ƒæ­å»º
```shell
- conda create -n python3.10 python=3.10ï¼ˆå»ºè®®ï¼‰
- pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
- cd */wenda/requirements/ 
- pip install -r requirements.txt # update transformers==4.27.1
- pip install requirements-glm6b-lora.txt
- pip install protobuf transformers==4.27.1 cpm_kernels
```
## coding
```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("./chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("./chatglm-6b-int4", trust_remote_code=True).half().cuda()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)

# ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
```

## pretrained model
- [é“¾æ¥](https://pan.baidu.com/s/1VPRGReHfnnqe_ULKjquoaQ?pwd=oaae)
- æå–ç ï¼šoaae 

# å‚è€ƒèµ„æ–™
[å‚è€ƒèµ„æ–™2](https://huggingface.co/THUDM/chatglm-6b-int4)
[å‚è€ƒèµ„æ–™](https://github.com/wenda-LLM/wenda/tree/main)